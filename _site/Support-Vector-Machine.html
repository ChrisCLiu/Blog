<!DOCTYPE html>
<html>

<head>
	<!-- Meta -->
	<meta charset="UTF-8"/>
	<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">
	<meta name="generator" content="Jekyll">

	<title>Support Vector Machine</title>
  <meta name="description" content="A blog for sharing thoughts on technology, science and insights at TheReviewIndex.">

	<!-- CSS & fonts -->
	<link rel="stylesheet" href="/css/main.css">

	<!-- RSS -->
	<link href="/atom.xml" type="application/atom+xml" rel="alternate" title="ATOM Feed" />

  <!-- Favicon -->
  <link rel="shortcut icon" type="image/png" href="img/favicon.png">

</head>


<body>
	<div id="wrap">
	  	
	  	<!-- Navigation -->
	  	<nav id="nav">
	<div id="nav-list">
		<a href="http://blog.xliu.info//">Home</a>

		<!-- Nav pages -->
	  
	    
	  
	    
	      <a href="/about" title="About">About</a>
	    
	  
	    
	  
	    
	  
	    
	  
	    
	  
	    
	  
    
    <!-- Nav links -->
	  <a href="http://xliu.info/">Personal Site</a>
<a target="_blank" href="https://github.com/ChrisCLiu/">Github</a>


	</div>
  
  <!-- Nav footer -->
	
	  <footer>
	
	<span>version 1.0.0</span>

</footer>
	

</nav>

    
    <!-- Icon menu -->
	  <a id="nav-menu">
	  	<div id="page-menu"></div>
	  </a>

      <!-- Header -->
      
        <header id="header-page" class="parent justify-spaceBetween">
  <div class="inner w100 relative">
    <span class="f-left">  
      <a href="http://blog.xliu.info/">
        <h1>
          Hope<span>from</span>Inside
        </h1>
      </a>
    </span>
    <span id="nav-links" class="absolute right bottom">
      <!-- Nav pages -->
	    
	      
	    
	      
	        <a href="/about" title="About">About</a>
	      
	    
	      
	    
	      
	    
	      
	    
	      
	    
	      
	    
      
      <!-- Nav links -->
	    <a href="http://xliu.info/">Personal Site</a>
<a target="_blank" href="https://github.com/ChrisCLiu/">Github</a>

    </span>
  </div>
</header>


      

    <!-- Main content -->
	  <div id="container">
		  
		<main>

			<article id="post-page">
	<h2>Support Vector Machine</h2>		
	<time datetime="2017-04-23T00:00:00+08:00" class="by-line">23 Apr 2017</time>
	<div class="content">

		<p>Support vector machine is a kind of advanced classification method. Basic SVM is used as a binary classification model. Its main propose is finding a linear expression of features and saperating datas into two classification as far as possible. That is, SVM tries to find a hyperplane that maximize the margin between the boundary of two data groups. The vector from the hyperplane to the data at the boundary is so-called support vector.</p>

<h3 id="relationship-to-logistic-regression">Relationship to Logistic Regression</h3>
<p>SVM is a linear classification model. We need to find a hyperplane $H$ such that</p>

<script type="math/tex; mode=display">H: \omega^T x + b = 0</script>

<p>We define $\hat y$ as the classification if $\omega^T x + b&lt;0$, $y$ is classified to -1 and if $\omega^T x + b&gt;0$, $y$ is classified to 1. This kind of classification is inherited from 0-1 classification from <a href="http://blog.xliu.info/Logistic-Regression">Logistic Regression</a>. -1-1 is used here for the convenience of calculaton.</p>

<p>We will compare the loss functions of LR and SVM in <a href="#hinge-loss">Hinge Loss</a></p>

<h3 id="a-simple-example-for-linear-classification">A Simple Example for Linear Classification.</h3>
<p>Again, we recall that the propose of SVM is to find a hyperplane $H$ so that could maxified the margin, which is so-called <em>Marximum Margin Classifier</em>. In the picture below, we showed in detail what is SVM</p>

<div align="center">
<img src="https://ooo.0o0.ooo/2017/04/23/58fc6c39569bf.png" alt="SVM.png" title="SVM.png" />
</div>

<p>As we can see in the picture, two group of items are located at different sides of the line $\omega^Tx+b = 0$, which is our hyperplane $H$ in this example. The dotted lines are paralleled to the hyperplane $H$, and they are the <em>classification boundary</em>. We call the data record located on the boundary as <strong><em>support vector</em></strong> and therefore the support vector satisfied the condition $y(\omega^Tx+b) = 1$. Our job in SVM is maxified the <em>Gap</em>, which is $\dfrac{2}{\omega^T\omega}$. As we can see, the larger the gap is, the further two classifications will be.</p>

<h3 id="cost-function--coefficient-estimation">Cost Function &amp; Coefficient Estimation</h3>
<p>Although the example above is a very simple explanation to SVM, but the propose is generalized. We have to maxified the gap to get the classification.</p>

<p>Therefore, our goal is</p>

<script type="math/tex; mode=display">\max\ \dfrac{1}{\mid\mid\omega\mid\mid}\quad s.t.\ y^{(i)}(\omega^Tx^{(i)}+b)\ge 1,\ i=1,2,...m</script>

<p>In order to get the maximum of $\dfrac{1}{\mid\mid\omega\mid\mid}$, we calculate the minimum of $\dfrac{1}{2}\mid\mid\omega\mid\mid^2$. Then our goal is</p>

<script type="math/tex; mode=display">\min\ \dfrac{1}{2}\mid\mid\omega\mid\mid^2\quad s.t.\ y^{(i)}(\omega^Tx^{(i)}+b)\ge 1,\ i=1,2,...m</script>

<p>(SVM中为了方便计算进行了一系列变换，从之前认定分类边界的值为1，到现在的最大化问题转变为最小化问题，都是为了之后进行计算时方便。至于为什么转成了二次最小化问题，还是为了求导方便。)</p>

<p>Then the problem has tranfered to a optimizing problem under certain constraints. To handle the constraints, we introduce <em>Lagrange Multiplier</em> to transfer the problem to its <strong><em>dual problem</em></strong>. The <strong><em>Lagrange function</em></strong> is defined as,</p>

<script type="math/tex; mode=display">\mathcal{L}(\omega,b,\alpha) = \dfrac{1}{2}\mid\mid\omega\mid\mid^2 - \sum\limits_{i=1}^{n}\alpha_i(y^{(i)}(\omega^Tx^{(i)}+b)-1)</script>

<p>Then our cost function is</p>

<script type="math/tex; mode=display">J(\omega) = \max \limits_{\alpha_i\ge 0}\  \mathcal{L}(\omega,b,\alpha)</script>

<p>Obviously, if one constraint is not statisfied, then $J(\omega)$ will be infinite. Now, we transfer our problem into its dual problem.</p>

<p>Original Problem,</p>

<script type="math/tex; mode=display">\min\limits_{\omega,b}J(\omega) = \min\limits_{\omega,b}\max\limits_{\alpha_i\ge 0}\  \mathcal{L}(\omega,b,\alpha) = p^*</script>

<p>Dual Problem,</p>

<script type="math/tex; mode=display">\max\limits_{\alpha_i\ge 0} \min\limits_{\omega,b}\ \mathcal{L}(\omega,b,\alpha) = d^*</script>

<p>$ p^{ * } $ and $d^{ * } $ is the optimization solution to original problem and dual problem, and we have relation $p^{ * } \ge \mathcal{L}(\omega,b,\alpha) \ge d^{ * }$.</p>

<p>We call $\Delta = p^{ * } - d^{ * } $ as <strong><em>duality gap</em></strong>, when $\Delta$ is 0, we call the problems have <strong><em>strong duality</em></strong>. If and only if at least one possible solution exsits, that is, we have strong duality, then the KKT condition must be satisfied.</p>

<h5 id="kkt-condition">KKT Condition</h5>
<blockquote>

  <p>If we have a constaint optimization problem defined as below,</p>

  <script type="math/tex; mode=display">\min f(x)\quad s.t.\  h_j(x) = 0,\ j=1,...,p\ \text{and}\ g_k(x) \le 0,\ k=1,...,q</script>

  <p>We define Lagrange function as,</p>

  <script type="math/tex; mode=display">\mathcal{L}(x,\lambda,\mu) = f(x) + \sum\limits_{j=1}^{p}\lambda_jh_j(x) + \sum\limits_{k=1}^{q}\mu_kg_k(x)</script>

  <p>The optimized solution $x^* $ should satisfy the following conditions,</p>

  <script type="math/tex; mode=display">\dfrac{\partial \mathcal{L}}{\partial x}\Bigg|_{x=x^*} = 0,</script>

  <script type="math/tex; mode=display">\lambda_j \neq 0, \quad \mu_k \ge 0,\ \text{and} \quad \mu_kg_k(x^*) = 0</script>

  <script type="math/tex; mode=display">h_j(x^* )=0,\ j=1,...,p \quad \text{and}\quad g_k(x^*) \le 0,\ k=1,...,q</script>
</blockquote>

<p>Come back to our lagrange function, we do a partial derivative to both $\omega$ and $b$, then we get</p>

<script type="math/tex; mode=display">\dfrac{\partial \mathcal{L}}{\partial \omega} = 0 \rightarrow \omega = \sum\limits_{i=1}^m \alpha_iy^{(i)}x^{(i)}</script>

<script type="math/tex; mode=display">\dfrac{\partial \mathcal{L}}{\partial b} = 0 \rightarrow \sum\limits_{i=1}^m \alpha_iy^{(i)} = 0</script>

<p>Apply them to larange function, we get</p>

<script type="math/tex; mode=display">\mathcal{L}(\omega,b,\alpha) = \sum\limits_{i=1}^m \alpha_i - \dfrac{1}{2}\sum\limits_{i,j=1}^m\alpha_i\alpha_jy^{(i)}y^{(j)}(x^{(i)})^Tx^{(j)}</script>

<p>Therefore, we have the final optimization problem only having $\alpha$ as parameter.</p>

<script type="math/tex; mode=display">\max\limits_{\alpha}\ \sum\limits_{i=1}^m \alpha_i - \dfrac{1}{2}\sum\limits_{i,j=1}^m\alpha_i\alpha_jy^{(i)}y^{(j)}(x^{(i)})^Tx^{(j)}</script>

<script type="math/tex; mode=display">s.t.\ \quad \alpha_i \ge 0,\ i = 1,...,m \  \quad \text{and} \quad \sum\limits_{i=1}^{m} \alpha_i y^{(i)} = 0</script>

<p>Once we get $\alpha$, we can easily calculate $\omega$ and $b$ by following expression</p>

<script type="math/tex; mode=display">\omega^* = \sum\limits_{i=1}^m \alpha_iy^{(i)}x^{(i)}</script>

<script type="math/tex; mode=display">b^* = \dfrac{ \max_{i:\ y^{(i)}=-1} \omega^{ * T} x^{(i)} + \min_{i:y^{(i)}=1}\omega^{ * T}x^{(i)}}{2}</script>

<h3 id="kernel-function--non-linearity">Kernel Function &amp; Non-Linearity</h3>
<p>Obviously, our SVM now can only handle the linear classification problem. When the data is not linear classified, we introduce kernel function $\kappa(·,·)$. kernel function map the low-dimentional data into a higher dimention, therefore, if map properly, the data will be linear claasifiable.</p>

<p>Take a simple example, say, we have a function $\phi(x)$ in $\mathbb{R}^5$. Data on $\mathbb{R}^2$ are distributed as two concentric circles, the features are $X_1$ and $X_2$. Then  $\varphi(X_1, X_2) = \sum\limits_{i=1}^5\alpha_iZ_i + \alpha_0 = 0$ where $Z_1 = X_1,\ Z_2 = X_2,\ Z_3 = X_1^2,\ Z_4 = X_2^2,$ and $Z_5 = X_1X_2$. Hence, the data becomes linear classifiable. Therefore, we only need to calculate $(\phi(x)^{(i)})^T\phi(x)^{(j)}$ instead of $(x^{(i)})^Tx^{(j)}$.</p>

<p>However, there is an obvious problem – the calculation. The map function will no doubt lead to a dimensional disaster. Therefore, we need kernel function.</p>

<p>Take a simple example. we assume $x_1 = (\eta_1, \eta_2)$ and $x_2 = {\xi_1,\xi_2}$. $\phi(·)$ is a map from $\mathbb{R}^2$ to $\mathbb{R}^5$ as introduced above.</p>

<p>$&lt;\phi(x_1),\phi(x_2)&gt; = \eta_1\xi_1 + \eta_1^2\xi_1^2 + \eta_2\xi_2 + \eta_2^2\xi_2^2 + \eta_1\eta_2\xi_1\xi_2$</p>

<p>we note that $\kappa(x_1,x_2) = (&lt; x_1,x_2 &gt;+1)^2 = 2\eta_1\xi_1 + \eta_1^2\xi_1^2 + 2\eta_2\xi_2 + \eta_2^2\xi_2^2 + 2\eta_1\eta_2\xi_1\xi_2 + 1$</p>

<p>Campare to the map $\varphi$, all we need to do is adjust some parameters and add a extra dimension 1.</p>

<p>This means that we could do the calculation in the low dimension and get the same result of the calculation in hign dimension with a proper kernel function! The kernel avoids the computation in high dimension and simplifies the calculation of inner product in it. There is another “coincidence” that all our calculation of $x$ is SVM is inner product!</p>

<p><strong><em>Gaussian kernel</em></strong> $\kappa(x_1,x_2) = exp(-\dfrac{\mid\mid x_1-x_2\mid\mid^2}{2\sigma^2})$ is a frequently used kernel function, it map to a infinite dimension space. A large $\sigma$ will punish the high dimension features hard and make it actually a low dimensional subspace. A small $\sigma$ will map every dataset to a linear classifiable space, certainly, it will bring a severe overfitting problem.</p>

<p><strong><em>Polynomial kernel</em></strong> $\kappa(x_1,x_2) = (&lt; x_1,x_2 &gt;+R)^d$ is another known kernel functionof degree $d$, it map to a $\binom{m+d}{m}$ dimensional space.</p>

<p>When a support vector classifier is combined with a non-linear kernel like the above two, the resulting classifier is known as a <strong><em>support vector machine</em></strong>.</p>

<h3 id="potential-extensions">Potential Extensions</h3>

<h4 id="slack-variables-to-handle-outliers">Slack Variables to Handle Outliers</h4>
<p>If the data records are not strictly seperated, say, some naughty data point located too far away from its group and too close to its opponents’ group, the origin SVM will suffer a bad result. Therefore, slack variables are introduced.</p>

<p>We still do a SVM but only ignore the naughty ones, that is we introduce $\xi_i$ as slack variables into the constraints. that is</p>

<script type="math/tex; mode=display">y^{(i)} (\omega^Tx^{(i)}+b) \ge 1-\xi_i</script>

<p>$\xi_i$ allows data point $x^{(i)}$ to be away from the boundary, but we still need them to be as small as possible, therefore, in origin goal, we need add one more term,</p>

<script type="math/tex; mode=display">\min\ \dfrac{1}{2}\mid\mid\omega\mid\mid^2 + C\sum\limits_{i=1}^m \xi_i\quad</script>

<script type="math/tex; mode=display">s.t.\ y^{(i)}(\omega^Tx^{(i)}+b)\ge 1-\xi_i,\ \text{and}\ \xi_i\ge 0 \quad i=1,2,...m</script>

<p>where $C$ is used to balance the weight between find the hyperplane and allow the error.</p>

<p>The lagrance function changes to</p>

<script type="math/tex; mode=display">\mathcal{L}(\omega,b,\xi,\alpha,r) = \dfrac{1}{2}\mid\mid\omega\mid\mid^2 + C\sum\limits_{i=1}^{m}\xi_i - \sum\limits_{i=1}^{n}\alpha_i(y^{(i)}(\omega^Tx^{(i)}+b)-1+\xi_i) - \sum\limits_{i=1}^{m}r_i\xi_i</script>

<p>Use same analyze strategy as before, we get</p>

<script type="math/tex; mode=display">\dfrac{\partial \mathcal{L}}{\partial \omega} = 0 \rightarrow \omega = \sum\limits_{i=1}^m \alpha_iy^{(i)}x^{(i)}</script>

<script type="math/tex; mode=display">\dfrac{\partial \mathcal{L}}{\partial b} = 0 \rightarrow \sum\limits_{i=1}^m \alpha_iy^{(i)} = 0</script>

<script type="math/tex; mode=display">\dfrac{\partial \mathcal{L}}{\partial \xi} = 0 \rightarrow C-\alpha_i-r_i = 0,\ i = 1,...,m</script>

<p>Because $C-\alpha_i-r_i = 0$ and $r_i\ge 0$ (as the request of being Lagrange multiplier) Then we get the dual problem</p>

<script type="math/tex; mode=display">\max\limits_{\alpha}\ \sum\limits_{i=1}^m \alpha_i - \dfrac{1}{2}\sum\limits_{i,j=1}^m\alpha_i\alpha_jy^{(i)}y^{(j)}(x^{(i)})^Tx^{(j)}</script>

<script type="math/tex; mode=display">s.t.\ \quad 0 \le \alpha_i \le C,\ i = 1,...,m \  \quad \text{and} \quad \sum\limits_{i=1}^{m} \alpha_i y^{(i)} = 0</script>

<h4 id="multi-classification">Multi-Classification</h4>
<p>Though many proposals for extending SVM to the $K$-class case have been made, the two most popular are the <strong><em>one-versus-one</em></strong> and <strong><em>one-verssus-all</em></strong>. Brief introduction will shows as below</p>

<h5 id="one-versus-one-classification">One-Versus-One Classification</h5>
<p>When there are $k$ classes, this method constructs $\binom{k}{2}$ SVMs, each of which compares a pair of classes, which is every time there are only two classes are choosed to compare. We classify a test observation using each of these SVMs and the final classification is performed by assigning the test observation to the class to which it was most frequently assigned in these pairwise classifications.</p>

<h5 id="one-versus-all-classification">One-Versus-All Classification</h5>
<p>When there are $k$ classes, this method constructs $k$ SVMs, each of which compares one against others, which is when one class is classified, anther $k-1$ classes are considered as a whole. We classify a test observation using each of these SVMs and the final classification is performed by finding out the cost-least-classification.</p>

<h3 id="hinge-loss">Hinge Loss</h3>
<p>The cost function in SVM is called <strong><em>Hinge Loss</em></strong>, represented as</p>

<script type="math/tex; mode=display">\max(0, 1-t_iy^{(i)})</script>

<p>where $t_i$ is the right label of the data point $x^{(i)}$. This loss function is closely related to <strong><em>logarithm loss</em></strong> used in logistic regression, which is represented as</p>

<script type="math/tex; mode=display">-log(P(Y\mid X))</script>

<p><em>Hinge loss</em> and <em>logarithm loss</em> are similar but when the data is classified as a right point, the former ignores its cost but the latter still counts as a very small value, the curve showed the cost when the right class is $y=1$</p>

<p>The comparasion showed as below,</p>
<div align="center">
<img src="https://ooo.0o0.ooo/2017/04/23/58fcb4ef5474e.png" alt="SVM_Loss.png" title="SVM_Loss.png" />
</div>

<h3 id="smo-algorithm">SMO Algorithm</h3>

<p>To be continue…</p>

		
	</div>
</article>



	  </main>
		
		  <!-- Pagination links -->
      

	  </div>
	    
	    <!-- Footer -->
	    <script type="text/javascript" src="/js/jquery-3.2.1.min.js"></script>
<script type="text/javascript">
$(document).ready(function(){
  $("totop").click(function () { 
        var speed = 500;//滑动的速度
        $('html,body').animate({ scrollTop: 0 }, speed);//去顶部
        //$('html,body').animate({ scrollTop: $('#footer').offset().top }, speed);//去底部 
});
});
</script>

<a href="#top" data-scroll><totop>Back to Top</totop></a>

<footer><span>&copy; 2017 - Xiao Liu <span class="love">❤</span> SJTU</footer>


	    <!-- Script -->
      <script src="/js/main.js"></script>	
<script type="text/x-mathjax-config"> 
 MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}}); 
 </script> 
 <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-98386998-1', 'auto');
  ga('send', 'pageview');

</script>
 
  <script type="text/x-mathjax-config"> 
 MathJax.Hub.Queue(function() { 
 var all = MathJax.Hub.getAllJax(), i; 
 for(i=0; i < all.length; i += 1) { 
 all[i].SourceElement().parentNode.className += ' has-jax'; 
 } 
 }); 
 </script> 
 
  <script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"> 
 </script> 

	</div>


</body>
</html>
